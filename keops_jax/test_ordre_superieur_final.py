import sys
sys.path.insert(0, '.')
from core.advanced_interface import jax_keops_convolution
import jax
import jax.numpy as jnp

print("=" * 60)
print("TEST FINAL - D√âRIV√âES D'ORDRE 3 ET 4")
print("=" * 60)

# Donn√©es tr√®s petites pour le test
key = jax.random.PRNGKey(42)
X = jax.random.normal(key, (2, 2), dtype=jnp.float32)  # Tr√®s petit!
Y = jax.random.normal(key, (3, 2), dtype=jnp.float32)
B = jnp.ones((3, 1), dtype=jnp.float32)

print(f"Donn√©es: X={X.shape}, Y={Y.shape}")
print(f"Taille entr√©e: {X.shape[0]*X.shape[1]} param√®tres")

# Fonction scalaire pour autodiff
def f(X):
    return jnp.sum(jax_keops_convolution("conv_gaussienne", X, Y, B))

print("\n1. Ordre 1: Gradient")
grad = jax.grad(f)(X)
print(f"   Shape: {grad.shape}")

print("\n2. Ordre 2: Hessienne")
hess = jax.hessian(f)(X)
print(f"   Shape: {hess.shape}")

print("\n3. Ordre 3: D√©riv√©e troisi√®me")
# jacobian du gradient
def grad_f(X):
    return jax.grad(f)(X)

third = jax.jacobian(grad_f)(X)
print(f"   Shape: {third.shape}")
print(f"   √âl√©ments: {third.size}")

print("\n4. Ordre 4: D√©riv√©e quatri√®me")
# hessian du gradient
fourth = jax.hessian(grad_f)(X)
print(f"   Shape: {fourth.shape}")
print(f"   √âl√©ments: {fourth.size}")

print("\n5. V√©rification")
# V√©rifie que tout est coh√©rent
M, D = X.shape
hess_flat = hess.reshape(M*D, M*D)
sym_err = jnp.linalg.norm(hess_flat - hess_flat.T)
print(f"   Erreur sym√©trie Hessienne: {sym_err:.2e}")

print("\n" + "üéâ" * 30)
if sym_err < 1e-4:
    print("SUCC√àS ABSOLU !")
    print(f"‚úì D√©riv√©es 1√®re ordre: {grad.shape}")
    print(f"‚úì D√©riv√©es 2√®me ordre: {hess.shape}") 
    print(f"‚úì D√©riv√©es 3√®me ordre: {third.shape}")
    print(f"‚úì D√©riv√©es 4√®me ordre: {fourth.shape}")
    print("\n‚úÖ KEOPS-JAX SUPPORTE L'AUTODIFF D'ORDRE SUP√âRIEUR !")
    print("‚úÖ Tu peux calculer des d√©riv√©es jusqu'au 4√®me ordre !")
else:
    print("Probl√®me avec les d√©riv√©es")
print("üéâ" * 30)
