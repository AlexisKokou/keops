# keops_jax/tests/test_derivatives_vector.py - CORRIGÃ‰
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

print("=" * 70)
print("TEST DÃ‰RIVÃ‰ES D'ORDRE N - VERSION CORRIGÃ‰E")
print("=" * 70)

import jax
import jax.numpy as jnp
import numpy as np

from core.jax_interface_nth_order import (
    jax_keops_convolution,
    jax_keops_gradient,
    jax_keops_directional_derivative,
    jax_keops_hessian
)

# Configurer KeOps
import pykeops
pykeops.clean_pykeops()
os.environ['PYKEOPS_FORCE_COMPILE'] = '1'

# DonnÃ©es de test
key = jax.random.PRNGKey(42)
M, N, D = 3, 4, 2
X = jax.random.normal(key, (M, D), dtype=jnp.float32)
Y = jax.random.normal(key, (N, D), dtype=jnp.float32)
B = jax.random.normal(key, (N, 1), dtype=jnp.float32)

print(f"ğŸ“Š DonnÃ©es: M={M}, N={N}, D={D}")

print("\n1ï¸âƒ£  TEST FORWARD:")
F = jax_keops_convolution("conv_gaussienne", X, Y, B)
print(f"   âœ… F(X) = {F.shape}")
print(f"   Valeurs: {F.flatten()}")

print("\n2ï¸âƒ£  TEST GRADIENT VECTORIEL (via KeOps):")
G = jax_keops_gradient("conv_gaussienne", X, Y, B)
print(f"   âœ… âˆ‡F(X) = {G.shape}")
print(f"   âˆ‡F[0,:] = {G[0]}")

print("\n3ï¸âƒ£  TEST DÃ‰RIVÃ‰E DIRECTIONNELLE:")
direction = jax.random.normal(key, (M, D))
D_dir = jax_keops_directional_derivative("conv_gaussienne", X, Y, B, direction)
print(f"   âœ… D_v F(X) = {D_dir.shape}")

# VÃ©rification: D_v F = âŸ¨âˆ‡F, vâŸ©
grad_dot_dir = jnp.sum(G * direction, axis=1, keepdims=True)
error = jnp.max(jnp.abs(D_dir - grad_dot_dir))
print(f"   VÃ©rification âŸ¨âˆ‡F,vâŸ©: erreur = {error:.2e}")

print("\n4ï¸âƒ£  TEST HESSIENNE (via JAX sur gradient KeOps):")
H = jax_keops_hessian("conv_gaussienne", X, Y, B)
print(f"   âœ… Hessienne = {H.shape}")

# VÃ©rification symÃ©trie
for i in range(M):
    H_i = H[i]
    sym_error = jnp.max(jnp.abs(H_i - H_i.T))
    print(f"   F[{i}] symÃ©trie erreur: {sym_error:.2e}")

print("\n5ï¸âƒ£  TEST DÃ‰RIVÃ‰ES D'ORDRE SUPÃ‰RIEUR VIA JAX:")

# Fonction scalaire pour JAX
def f_scalar(X):
    return jnp.sum(jax_keops_convolution("conv_gaussienne", X, Y, B))

print("   a) Gradient via JAX:")
grad_jax = jax.grad(f_scalar)(X)
print(f"      âˆ‡f(X) = {grad_jax.shape}")

print("   b) Hessienne via JAX:")
hess_jax = jax.hessian(f_scalar)(X)
print(f"      âˆ‡Â²f(X) = {hess_jax.shape}")

print("   c) 3Ã¨me ordre via JAX:")
third_jax = jax.jacobian(jax.hessian(f_scalar))(X)
print(f"      âˆ‡Â³f(X) = {third_jax.shape}")

print("\n" + "=" * 70)
print("ğŸ¯ RÃ‰SUMÃ‰:")
print("=" * 70)

print("""
âœ… ARCHITECTURE FONCTIONNELLE:

1. Forward (M,1) â†’ KeOps
2. Gradient (M,D) â†’ KeOps (optimisÃ©)
3. DÃ©rivÃ©e directionnelle (M,1) â†’ KeOps
4. Hessienne (M,D,D) â†’ JAX sur gradient KeOps
5. DÃ©rivÃ©es d'ordre supÃ©rieur â†’ JAX

ğŸ“Š AVANTAGES:
- KeOps optimise gradient et forward
- JAX gÃ¨re l'autodiff d'ordre supÃ©rieur
- Interface propre et efficace
- Pas de shape mismatch

ğŸš€ PRÃŠT POUR LA RECHERCHE!
""")