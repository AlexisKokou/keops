#!/usr/bin/env python3
"""
üöÄ PREUVE : D√©riv√©e 3√®me ordre NON NULLE avec KeOps-JAX
Montre des valeurs concr√®tes non nulles
"""

import sys
import os
sys.path.insert(0, os.path.join(os.getcwd(), ".."))

from keops_jax import keops_gaussian
import jax
import jax.numpy as jnp

print("=" * 70)
print("üî• PREUVE : D√âRIV√âE 3√àME ORDRE NON NULLE AVEC KEOPS-JAX")
print("=" * 70)

# 1. Configuration MINI pour voir les valeurs
key = jax.random.PRNGKey(42)

# Un SEUL point en 1D pour simplifier l'affichage
M, N, D = 1, 2, 1  # 1 point en 1D, 2 points de r√©f√©rence
X = jax.random.normal(key, (M, D), dtype=jnp.float32) * 2.0
Y = jax.random.normal(key, (N, D), dtype=jnp.float32) 
B = jnp.ones((N, 1), dtype=jnp.float32)

print(f"Donn√©es MINIMALES pour voir tout :")
print(f"  X (1 point 1D): {X.flatten()}")
print(f"  Y (2 points 1D): {Y.flatten()}")
print(f"  B: {B.flatten()}")
print()

# 2. Fonction qui ne d√©pend que de X (un seul point)
def f(X):
    return keops_gaussian(X, Y, B)

# 3. Calcul de TOUTES les d√©riv√©es
print("1. CALCUL DES D√âRIV√âES SUCCESSIVES :")
print("-" * 50)

# Valeur de la fonction
f_value = f(X)
print(f"f(X) = {f_value:.8f}")
print()

# Gradient (1√®re d√©riv√©e) - forme (1, 1)
grad1 = jax.grad(f)(X)
print("2. GRADIENT (1√®re d√©riv√©e) - NON NULL :")
print(grad1)
print(f"Valeur: {grad1[0, 0]:.8f}")
print(f"Norme: {jnp.linalg.norm(grad1):.8f}")
print()

# Hessienne (2√®me d√©riv√©e) - forme (1, 1, 1, 1) pour notre cas simple
hess = jax.hessian(f)(X)
print("3. HESSIENNE (2√®me d√©riv√©e) - NON NULLE :")
print(f"Forme: {hess.shape}")
print(f"Valeur: {hess[0, 0, 0, 0]:.8f}")
print(f"Norme: {jnp.linalg.norm(hess):.8f}")
print()

# 4. D√©riv√©e 3√®me ordre - C'EST LA QUE √áA COMPTE !
print("4. D√âRIV√âE 3√àME ORDRE - PREUVE QU'ELLE N'EST PAS NULLE !")
print("-" * 50)

# M√©thode 1: jacobian du gradient
def grad_func(X):
    return jax.grad(f)(X)

grad3 = jax.jacobian(grad_func)(X)
print(f"Forme du tenseur d'ordre 3: {grad3.shape}")

# Pour X de forme (1,1), grad3 est de forme (1,1,1,1,1,1)
# Afficher toutes les valeurs
print("\nValeurs du tenseur de d√©riv√©e 3√®me (6D):")
print("=" * 50)

# R√©cup√©rer toutes les valeurs
grad3_flat = grad3.flatten()
for i, val in enumerate(grad3_flat):
    print(f"  grad3[{i}] = {val:.10f}")

print(f"\nNorme L2 du tenseur d'ordre 3: {jnp.linalg.norm(grad3_flat):.10f}")
print(f"Maximum absolu: {jnp.max(jnp.abs(grad3_flat)):.10f}")
print(f"Minimum absolu: {jnp.min(jnp.abs(grad3_flat)):.10f}")
print()

# 5. V√©rification avec une perturbation
print("5. V√âRIFICATION PRATIQUE AVEC PERTURBATION :")
print("-" * 50)

# Direction de test
v = jnp.array([[0.1]], dtype=jnp.float32)  # petite perturbation en 1D

# Calcul de f(X + Œµv) pour plusieurs Œµ
epsilons = [0.0, 0.001, 0.01, 0.1]
print("\nD√©veloppement de Taylor √† l'ordre 3 :")
print("Œµ      | f(X+Œµv)     | Pr√©diction ordre 3 | Erreur")
print("-" * 50)

for eps in epsilons:
    X_pert = X + eps * v
    
    # Valeur exacte
    f_exact = f(X_pert)
    
    # Pr√©diction par d√©veloppement de Taylor
    f0 = f(X)
    grad_val = jnp.sum(grad1 * v)
    hess_val = jnp.sum(v[:, :, None, None] * hess * v[None, None, :, :])
    grad3_val = jnp.sum(v[:, :, None, None, None, None] * grad3 * 
                        v[None, None, :, :, None, None] * 
                        v[None, None, None, None, :, :])
    
    f_pred = f0 + eps*grad_val + (eps**2)/2 * hess_val + (eps**3)/6 * grad3_val
    
    error = jnp.abs(f_exact - f_pred)
    print(f"{eps:6.3f} | {f_exact:.8f} | {f_pred:.8f}       | {error:.2e}")

print()

# 6. Comparaison avec JAX pur pour v√©rifier
print("6. COMPARAISON AVEC JAX PUR (M√äME CALCUL) :")
print("-" * 50)

# Fonction JAX pure √©quivalente
def gaussian_jax_pure(X, Y, B):
    diff = X[:, None, :] - Y[None, :, :]
    sq_dist = jnp.sum(diff**2, axis=-1)
    K = jnp.exp(-sq_dist)
    return jnp.sum(K @ B)

def f_jax(X):
    return gaussian_jax_pure(X, Y, B)

# Calcul des d√©riv√©es avec JAX pur
grad1_jax = jax.grad(f_jax)(X)
hess_jax = jax.hessian(f_jax)(X)
grad3_jax = jax.jacobian(jax.grad(f_jax))(X)

print("\nComparaison des normes :")
print(f"               | KeOps-JAX       | JAX pur         | Diff√©rence")
print("-" * 60)
print(f"Gradient       | {jnp.linalg.norm(grad1):.10f} | {jnp.linalg.norm(grad1_jax):.10f} | {jnp.linalg.norm(grad1 - grad1_jax):.2e}")
print(f"Hessienne      | {jnp.linalg.norm(hess):.10f} | {jnp.linalg.norm(hess_jax):.10f} | {jnp.linalg.norm(hess - hess_jax):.2e}")
print(f"D√©riv√©e 3√®me   | {jnp.linalg.norm(grad3):.10f} | {jnp.linalg.norm(grad3_jax):.10f} | {jnp.linalg.norm(grad3 - grad3_jax):.2e}")

print()

# 7. Affichage des valeurs BRUTES pour convaincre
print("7. VALEURS BRUTES POUR CONVAINCRE :")
print("-" * 50)
print("\nD√âRIV√âE 3√àME - √âL√âMENTS NON NULLS :")
print("Indice | Valeur KeOps-JAX | Valeur JAX pur | Diff√©rence")
print("-" * 60)

# Afficher les 10 premiers √©l√©ments
grad3_flat = grad3.flatten()
grad3_jax_flat = grad3_jax.flatten()

for i in range(min(10, len(grad3_flat))):
    val_k = grad3_flat[i]
    val_j = grad3_jax_flat[i]
    diff = jnp.abs(val_k - val_j)
    print(f"{i:6d} | {val_k:16.10f} | {val_j:16.10f} | {diff:.2e}")

# Trouver l'√©l√©ment avec la plus grande valeur absolue
max_idx = jnp.argmax(jnp.abs(grad3_flat))
max_val_k = grad3_flat[max_idx]
max_val_j = grad3_jax_flat[max_idx]

print(f"\n√âl√©ment max (indice {max_idx}):")
print(f"  KeOps-JAX: {max_val_k:.10f}")
print(f"  JAX pur:   {max_val_j:.10f}")
print(f"  Diff√©rence: {jnp.abs(max_val_k - max_val_j):.2e}")

print()

# 8. Test avec un cas PLUS INT√âRESSANT (2D)
print("8. CAS PLUS INT√âRESSANT : 2 POINTS EN 2D")
print("-" * 50)

M, N, D = 2, 3, 2  # 2 points en 2D
X2 = jax.random.normal(key, (M, D), dtype=jnp.float32)
Y2 = jax.random.normal(key, (N, D), dtype=jnp.float32) 
B2 = jnp.ones((N, 1), dtype=jnp.float32)

def f2(X):
    return keops_gaussian(X, Y2, B2)

# Calcul de la d√©riv√©e 3√®me
grad3_2 = jax.jacobian(jax.grad(f2))(X2)

print(f"\nForme de la d√©riv√©e 3√®me pour X({M},{D}): {grad3_2.shape}")
print(f"Norme L2: {jnp.linalg.norm(grad3_2.flatten()):.10f}")

# √âchantillon de valeurs non nulles
flat_grad3_2 = grad3_2.flatten()
non_zero_indices = jnp.where(jnp.abs(flat_grad3_2) > 1e-10)[0]

print(f"\nNombre d'√©l√©ments non nuls (|val| > 1e-10): {len(non_zero_indices)}/{len(flat_grad3_2)}")
print("\nQuelques valeurs non nulles :")
for i in range(min(5, len(non_zero_indices))):
    idx = non_zero_indices[i]
    val = flat_grad3_2[idx]
    print(f"  Element {idx}: {val:.10f}")

print()

print("=" * 70)
print("‚úÖ CONCLUSION : LES D√âRIV√âES D'ORDRE 3 SONT BIEN NON NULLES !")
print("=" * 70)
print()
print("üìä PREUVES APPORT√âES :")
print("1. Valeurs brutes affich√©es (toutes non nulles)")
print("2. Normes non nulles")
print("3. Comparaison avec JAX pur (m√™mes valeurs)")
print("4. D√©veloppement de Taylor qui converge")
print("5. Cas 2D avec nombreux √©l√©ments non nuls")
print()
print("üéØ POUR LES SCEPTIQUES :")
print("La d√©riv√©e 3√®me d'un noyau gaussien n'est PAS nulle.")
print("C'est math√©matiquement impossible car la gaussienne est")
print("infiniment diff√©rentiable et toutes ses d√©riv√©es existent.")
print()
print("KeOps-JAX pr√©serve cette propri√©t√© gr√¢ce √† l'autodiff de JAX !")